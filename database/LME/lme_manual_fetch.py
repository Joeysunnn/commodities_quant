"""
LME æ‰‹åŠ¨è·å–å·¥å…·
- é€‰æ‹©ç‰¹å®šæ—¥æœŸèŒƒå›´ä¸‹è½½æŠ¥å‘Š
- è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±çš„æŠ¥å‘Š
- æå–å¹¶æ¸…æ´—é“œ(Copper)æ•°æ®
- è½¬æ¢ä¸ºæ ‡å‡†åŒ–é•¿æ ¼å¼å¹¶å­˜å…¥æ•°æ®åº“
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import requests
import os
import sys
import time
import re
import hashlib
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
from urllib.parse import urljoin

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, ROOT_DIR)

from db_utils import DatabaseSession


class LMEManualFetcher:
    """LMEæ•°æ®æ‰‹åŠ¨è·å–ç±»"""
    
    def __init__(self, download_folder="lme_reports", headless=False):
        self.base_url = "https://www.lme.com"
        self.script_dir = os.path.dirname(os.path.abspath(__file__))
        self.download_folder = os.path.join(self.script_dir, download_folder)
        
        # åˆ›å»ºä¸‹è½½æ–‡ä»¶å¤¹
        if not os.path.exists(self.download_folder):
            os.makedirs(self.download_folder)
        
        # è®¾ç½®Chromeé€‰é¡¹
        chrome_options = Options()
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--window-size=1920,1080')
        
        if headless:
            chrome_options.add_argument('--headless')
        
        # é…ç½®Chromeè‡ªåŠ¨ä¸‹è½½
        prefs = {
            "download.default_directory": self.download_folder,
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "safebrowsing.enabled": True
        }
        chrome_options.add_experimental_option("prefs", prefs)
        
        try:
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    })
                '''
            })
            print("Chromeæµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
        except Exception as e:
            print(f"å¯åŠ¨Chromeå¤±è´¥: {e}")
            raise
        
        # åˆ›å»ºrequests sessionç”¨äºä¸‹è½½
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def close_cookie_popup(self):
        """å…³é—­cookieå¼¹çª—"""
        try:
            cookie_buttons = [
                (By.ID, "onetrust-accept-btn-handler"),
                (By.CLASS_NAME, "onetrust-close-btn-handler"),
                (By.XPATH, "//button[contains(text(), 'Accept')]"),
            ]
            
            for by, value in cookie_buttons:
                try:
                    button = WebDriverWait(self.driver, 2).until(
                        EC.element_to_be_clickable((by, value))
                    )
                    button.click()
                    print("å·²å…³é—­cookieå¼¹çª—")
                    time.sleep(1)
                    return
                except:
                    continue
        except Exception:
            pass
    
    def get_page(self, url):
        """ä½¿ç”¨Seleniumè·å–é¡µé¢å†…å®¹"""
        try:
            print(f"æ­£åœ¨è®¿é—®: {url}")
            self.driver.get(url)
            time.sleep(3)
            self.close_cookie_popup()
            
            # é€æ­¥æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ‡’åŠ è½½å†…å®¹
            print("æ­£åœ¨é€æ­¥æ»šåŠ¨é¡µé¢åŠ è½½æ‰€æœ‰æŠ¥å‘Š...")
            
            # è·å–é¡µé¢æ€»é«˜åº¦
            total_height = self.driver.execute_script("return document.body.scrollHeight")
            current_position = 0
            scroll_step = 600  # æ¯æ¬¡æ»šåŠ¨600åƒç´ 
            scroll_pause = 1.5  # æ¯æ¬¡æ»šåŠ¨åæš‚åœ1.5ç§’
            
            # é€æ­¥å‘ä¸‹æ»šåŠ¨
            while current_position < total_height:
                # æ»šåŠ¨æŒ‡å®šè·ç¦»
                current_position += scroll_step
                self.driver.execute_script(f"window.scrollTo(0, {current_position});")
                time.sleep(scroll_pause)
                
                # é‡æ–°è·å–é¡µé¢é«˜åº¦ï¼ˆå› ä¸ºæ‡’åŠ è½½å¯èƒ½å¢åŠ é¡µé¢é«˜åº¦ï¼‰
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                if new_height > total_height:
                    total_height = new_height
                    print(f"  æ£€æµ‹åˆ°æ–°å†…å®¹ï¼Œé¡µé¢é«˜åº¦: {total_height}px")
            
            print(f"  æ»šåŠ¨å®Œæˆï¼Œå…±æ»šåŠ¨è‡³ {current_position}px")
            
            # æ»šåŠ¨å›é¡¶éƒ¨
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(1)
            
            try:
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CLASS_NAME, "search-results__page"))
                )
            except TimeoutException:
                pass
            
            return self.driver.page_source
        except Exception as e:
            print(f"è·å–é¡µé¢å¤±è´¥: {e}")
            return None
    
    def parse_report_cards(self, soup):
        """è§£ææŠ¥å‘Šå¡ç‰‡"""
        reports = []
        search_results = soup.find('ul', class_='search-results__page')
        
        if not search_results:
            return reports
        
        report_cards = search_results.find_all('li', class_='report-card')
        
        for card in report_cards:
            try:
                title_link = card.find('a', class_='report-card__title-link')
                if not title_link:
                    continue
                
                title = title_link.get_text(strip=True)
                href = title_link.get('href', '')
                
                if not href:
                    download_btn = card.find('a', class_='button-secondary')
                    if download_btn:
                        href = download_btn.get('href', '')
                
                if not href:
                    continue
                
                file_url = href if href.startswith('http') else urljoin(self.base_url, href)
                
                reports.append({
                    'title': title,
                    'url': file_url
                })
                
            except Exception as e:
                continue
        
        return reports
    
    def parse_date_from_title(self, title):
        """ä»æŠ¥å‘Šæ ‡é¢˜ä¸­è§£ææ—¥æœŸ"""
        match = re.search(r'(\d{1,2})\s+(\w+)\s+(\d{4})', title)
        if not match:
            return None
        
        day = int(match.group(1))
        month_str = match.group(2)
        year = int(match.group(3))
        
        month_map = {
            'jan': 1, 'january': 1, 'feb': 2, 'february': 2,
            'mar': 3, 'march': 3, 'apr': 4, 'april': 4,
            'may': 5, 'jun': 6, 'june': 6,
            'jul': 7, 'july': 7, 'aug': 8, 'august': 8,
            'sep': 9, 'september': 9, 'oct': 10, 'october': 10,
            'nov': 11, 'november': 11, 'dec': 12, 'december': 12,
        }
        
        month = month_map.get(month_str.lower())
        if not month:
            return None
        
        try:
            return datetime(year, month, day)
        except ValueError:
            return None
    
    def get_local_downloaded_files(self):
        """è·å–æœ¬åœ°å·²ä¸‹è½½çš„æŠ¥å‘Šæ–‡ä»¶åé›†åˆ"""
        downloaded = set()
        if not os.path.exists(self.download_folder):
            return downloaded
        
        for filename in os.listdir(self.download_folder):
            if filename.endswith('.xls'):
                downloaded.add(filename)
        
        return downloaded
    
    def get_months_in_range(self, start_date, end_date):
        """è·å–æ—¥æœŸèŒƒå›´å†…çš„æ‰€æœ‰æœˆä»½"""
        months = []
        current = start_date.replace(day=1)
        end_month = end_date.replace(day=1)
        
        while current <= end_month:
            months.append((str(current.year), current.strftime('%B')))
            current += relativedelta(months=1)
        
        return months
    
    def click_next_page(self):
        """ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®è¿›è¡Œç¿»é¡µ"""
        try:
            # å°è¯•å¤šç§æ–¹å¼æ‰¾åˆ°å¹¶ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®
            next_selectors = [
                "a.pagination__next",
                "a[rel='next']",
                "li.pagination__item--next a",
                "a[aria-label='Next']",
                "button.pagination__next",
            ]
            
            for selector in next_selectors:
                try:
                    next_btn = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if next_btn and next_btn.is_displayed() and next_btn.is_enabled():
                        # æ»šåŠ¨åˆ°æŒ‰é’®ä½ç½®
                        self.driver.execute_script("arguments[0].scrollIntoView(true);", next_btn)
                        time.sleep(0.5)
                        next_btn.click()
                        time.sleep(2)  # ç­‰å¾…é¡µé¢åŠ è½½
                        return True
                except:
                    continue
            
            # å°è¯•é€šè¿‡XPathæŸ¥æ‰¾
            xpath_selectors = [
                "//a[contains(@class, 'pagination') and contains(@class, 'next')]",
                "//a[contains(text(), 'Next')]",
                "//a[contains(text(), 'â€º')]",
                "//button[contains(text(), 'Next')]",
            ]
            
            for xpath in xpath_selectors:
                try:
                    next_btn = self.driver.find_element(By.XPATH, xpath)
                    if next_btn and next_btn.is_displayed() and next_btn.is_enabled():
                        self.driver.execute_script("arguments[0].scrollIntoView(true);", next_btn)
                        time.sleep(0.5)
                        next_btn.click()
                        time.sleep(2)
                        return True
                except:
                    continue
            
            return False
        except Exception as e:
            print(f"    ç¿»é¡µå¤±è´¥: {e}")
            return False
    
    def fetch_reports_in_range(self, start_date, end_date):
        """è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æŠ¥å‘Šï¼ˆæ”¯æŒåˆ†é¡µï¼‰"""
        
        # è·å–éœ€è¦æ£€æŸ¥çš„æ‰€æœ‰æœˆä»½
        months_to_check = self.get_months_in_range(start_date, end_date)
        
        print(f"\n{'='*60}")
        print(f"è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„æŠ¥å‘Š")
        print(f"æ—¥æœŸèŒƒå›´: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
        print(f"éœ€è¦æ£€æŸ¥çš„æœˆä»½: {months_to_check}")
        print(f"{'='*60}")
        
        all_reports = []
        seen_titles = set()  # ç”¨äºå»é‡
        
        for year, month in months_to_check:
            base_url = f"{self.base_url}/Market-data/Reports-and-data/Warehouse-and-stocks-reports/Stock-breakdown-report"
            
            print(f"\næ­£åœ¨è·å– {month} {year} çš„æŠ¥å‘Š...")
            
            # é¦–å…ˆè®¿é—®ç¬¬ä¸€é¡µ
            url = f"{base_url}?page=1&facetFilterPairs=report_friendly_date%2C{month}+{year}"
            
            html = self.get_page(url)
            if not html:
                continue
            
            page_num = 1
            month_total_reports = 0
            max_pages = 5  # å®‰å…¨é™åˆ¶ï¼Œé˜²æ­¢æ— é™å¾ªç¯
            
            while page_num <= max_pages:
                print(f"  ğŸ“„ ç¬¬ {page_num} é¡µ...")
                
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                page_reports = self.parse_report_cards(soup)
                
                if not page_reports:
                    if page_num == 1:
                        print(f"  æœªæ‰¾åˆ°æŠ¥å‘Š")
                    break
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æŠ¥å‘Šï¼ˆå»é‡æ£€æµ‹ï¼‰
                new_reports_count = 0
                for report in page_reports:
                    if report['title'] not in seen_titles:
                        new_reports_count += 1
                
                if new_reports_count == 0 and page_num > 1:
                    print(f"    æ£€æµ‹åˆ°é‡å¤å†…å®¹ï¼Œç¿»é¡µå¯èƒ½å¤±è´¥ï¼Œåœæ­¢å½“å‰æœˆä»½")
                    break
                
                print(f"    æ‰¾åˆ° {len(page_reports)} ä¸ªæŠ¥å‘Š (æ–°å¢ {new_reports_count} ä¸ª)")
                month_total_reports += new_reports_count
                
                # ç­›é€‰æ—¥æœŸèŒƒå›´å†…çš„æŠ¥å‘Š
                for report in page_reports:
                    if report['title'] in seen_titles:
                        continue
                    seen_titles.add(report['title'])
                    
                    report_date = self.parse_date_from_title(report['title'])
                    if report_date:
                        report['date'] = report_date
                        if start_date <= report_date <= end_date:
                            all_reports.append(report)
                            print(f"      âœ“ {report['title']}")
                        else:
                            print(f"      - è·³è¿‡(è¶…å‡ºèŒƒå›´): {report['title']}")
                
                # å¦‚æœå½“å‰é¡µæŠ¥å‘Šæ•°å°‘äº10ä¸ªï¼Œè¯´æ˜æ˜¯æœ€åä¸€é¡µ
                if len(page_reports) < 10:
                    print(f"    å½“å‰é¡µæŠ¥å‘Šæ•° < 10ï¼Œå·²åˆ°æœ€åä¸€é¡µ")
                    break
                
                # å°è¯•ç‚¹å‡»ä¸‹ä¸€é¡µ
                print(f"    å°è¯•ç¿»åˆ°ç¬¬ {page_num + 1} é¡µ...")
                if self.click_next_page():
                    page_num += 1
                    time.sleep(1)
                else:
                    print(f"    æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®æˆ–å·²æ˜¯æœ€åä¸€é¡µ")
                    break
            
            print(f"  âœ… {month} {year} å…± {page_num} é¡µ, {month_total_reports} ä¸ªæŠ¥å‘Š")
        
        all_reports.sort(key=lambda x: x.get('date', datetime.min), reverse=True)
        return all_reports
    
    def sanitize_filename(self, filename):
        """æ¸…ç†æ–‡ä»¶å"""
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        return filename
    
    def download_file(self, url, filename):
        """ä¸‹è½½æ–‡ä»¶"""
        try:
            print(f"  æ­£åœ¨ä¸‹è½½: {filename}")
            
            # è·å–seleniumçš„cookies
            cookies = self.driver.get_cookies()
            for cookie in cookies:
                self.session.cookies.set(cookie['name'], cookie['value'])
            
            # æ·»åŠ å®Œæ•´çš„è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': 'https://www.lme.com/Market-data/Reports-and-data/Warehouse-and-stocks-reports/Stock-breakdown-report',
                'Origin': 'https://www.lme.com',
                'Connection': 'keep-alive',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'same-origin',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1',
            }
            
            response = self.session.get(url, headers=headers, stream=True, timeout=30)
            response.raise_for_status()
            
            filepath = os.path.join(self.download_folder, filename)
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"  âœ“ ä¸‹è½½æˆåŠŸ: {filename}")
            return True
        except Exception as e:
            print(f"  âœ— ä¸‹è½½å¤±è´¥: {filename}, é”™è¯¯: {e}")
            # å¦‚æœrequestså¤±è´¥ï¼Œå°è¯•ä½¿ç”¨seleniumç›´æ¥ä¸‹è½½
            return self.download_with_selenium(url, filename)
    
    def download_with_selenium(self, url, filename):
        """ä½¿ç”¨Seleniumç›´æ¥ä¸‹è½½æ–‡ä»¶ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        try:
            print(f"  å°è¯•ä½¿ç”¨æµè§ˆå™¨ç›´æ¥ä¸‹è½½...")
            
            # è®°å½•ä¸‹è½½å‰çš„æ–‡ä»¶åˆ—è¡¨
            before_files = set(os.listdir(self.download_folder))
            
            # ä½¿ç”¨seleniumè®¿é—®ä¸‹è½½é“¾æ¥
            self.driver.get(url)
            
            # ç­‰å¾…ä¸‹è½½å®Œæˆï¼ˆæœ€å¤šç­‰å¾…30ç§’ï¼‰
            for i in range(30):
                time.sleep(1)
                after_files = set(os.listdir(self.download_folder))
                new_files = after_files - before_files
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ–‡ä»¶ï¼ˆæ’é™¤.crdownloadä¸´æ—¶æ–‡ä»¶ï¼‰
                completed_files = [f for f in new_files if not f.endswith('.crdownload')]
                
                if completed_files:
                    downloaded_file = completed_files[0]
                    print(f"  âœ“ æµè§ˆå™¨ä¸‹è½½æˆåŠŸ: {downloaded_file}")
                    
                    # å¦‚æœéœ€è¦é‡å‘½å
                    if downloaded_file != filename:
                        old_path = os.path.join(self.download_folder, downloaded_file)
                        new_path = os.path.join(self.download_folder, filename)
                        try:
                            if os.path.exists(new_path):
                                os.remove(new_path)
                            os.rename(old_path, new_path)
                            print(f"  å·²é‡å‘½åä¸º: {filename}")
                        except:
                            pass
                    return True
                
                # æ£€æŸ¥æ˜¯å¦è¿˜åœ¨ä¸‹è½½ä¸­
                downloading = [f for f in new_files if f.endswith('.crdownload')]
                if downloading:
                    continue
            
            print(f"  ä¸‹è½½è¶…æ—¶")
            return False
                
        except Exception as e:
            print(f"  Seleniumä¸‹è½½ä¹Ÿå¤±è´¥: {e}")
            return False
    
    def extract_copper_from_report(self, filepath):
        """ä»å•ä¸ªæŠ¥å‘Šä¸­æå–é“œæ•°æ®"""
        try:
            # è¯»å–Excelæ–‡ä»¶
            df = pd.read_excel(filepath, sheet_name=0, header=None)
            
            # æ‰¾åˆ°Copperæ•°æ®åŒºåŸŸ
            copper_start_row = None
            for idx, row in df.iterrows():
                row_str = ' '.join([str(cell).lower() for cell in row if pd.notna(cell)])
                if 'copper' in row_str:
                    copper_start_row = idx
                    break
            
            if copper_start_row is None:
                return None
            
            # æ‰¾åˆ°è¡¨å¤´è¡Œ
            header_row = None
            for idx in range(copper_start_row, min(copper_start_row + 10, len(df))):
                row_str = ' '.join([str(cell).lower() for cell in df.iloc[idx] if pd.notna(cell)])
                if 'country' in row_str or 'region' in row_str or 'opening stock' in row_str:
                    header_row = idx
                    break
            
            if header_row is None:
                header_row = copper_start_row + 2
            
            # æ‰¾åˆ°Totalè¡Œï¼ˆæ•°æ®ç»“æŸï¼‰
            total_row = None
            for idx in range(header_row + 1, min(header_row + 50, len(df))):
                first_cell = str(df.iloc[idx, 0]).strip().lower()
                if first_cell == 'total':
                    total_row = idx
                    break
            
            if total_row is None:
                return None
            
            # æå–Totalè¡Œçš„æ•°æ®
            total_data = df.iloc[total_row].tolist()
            
            # è§£ææ•°å€¼ï¼ˆé€šå¸¸æ˜¯ï¼š[Total, Opening, In, Out, Closing, Open, Cancelled]ï¼‰
            values = []
            for cell in total_data[1:]:  # è·³è¿‡ç¬¬ä¸€åˆ—ï¼ˆTotalæ ‡ç­¾ï¼‰
                try:
                    if pd.notna(cell):
                        values.append(float(cell))
                except:
                    continue
            
            if len(values) >= 6:
                return {
                    'Opening_Stock': values[0],
                    'Delivered_In': values[1],
                    'Delivered_Out': values[2],
                    'Closing_Stock': values[3],
                    'Open_Tonnage': values[4],
                    'Cancelled_Tonnage': values[5]
                }
            
            return None
            
        except Exception as e:
            print(f"  æå–æ•°æ®å¤±è´¥: {e}")
            return None
    
    def parse_date_from_filename(self, filename):
        """ä»æ–‡ä»¶åä¸­è§£ææ—¥æœŸ"""
        patterns = [
            r'Metals Reports (\d{2}) (\w+) (\d{4})\.xls',
            r'Metals-Reports-(\d{2})-(\w+)-(\d{4})\.xls',
        ]
        
        month_map = {
            'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',
            'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',
            'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12',
        }
        
        for pattern in patterns:
            match = re.search(pattern, filename, re.IGNORECASE)
            if match:
                day = match.group(1)
                month_str = match.group(2)
                year = match.group(3)
                month = month_map.get(month_str, month_map.get(month_str.capitalize()))
                if month:
                    return f"{year}{month}{day}"
        
        return None
    
    def process_downloaded_reports(self, start_date=None, end_date=None):
        """å¤„ç†ä¸‹è½½çš„æŠ¥å‘Šå¹¶ç”Ÿæˆæ±‡æ€»æ•°æ®ï¼ˆå¯é€‰ï¼šä»…å¤„ç†æŒ‡å®šæ—¥æœŸèŒƒå›´ï¼‰"""
        files = [f for f in os.listdir(self.download_folder) if f.endswith('.xls')]
        
        all_data = []
        
        for filename in files:
            date_str = self.parse_date_from_filename(filename)
            if not date_str:
                continue
            
            # å¦‚æœæŒ‡å®šäº†æ—¥æœŸèŒƒå›´ï¼Œåˆ™è¿‡æ»¤
            if start_date and end_date:
                try:
                    file_date = datetime.strptime(date_str, '%Y%m%d')
                    if not (start_date <= file_date <= end_date):
                        continue
                except:
                    continue
            
            filepath = os.path.join(self.download_folder, filename)
            copper_data = self.extract_copper_from_report(filepath)
            
            if copper_data:
                copper_data['Date'] = date_str
                all_data.append(copper_data)
                print(f"  âœ“ æå–æˆåŠŸ: {filename}")
            else:
                print(f"  âœ— æå–å¤±è´¥: {filename}")
        
        if not all_data:
            return None
        
        df = pd.DataFrame(all_data)
        df = df.sort_values('Date')
        
        return df
    
    def calculate_checksum(self, filepath):
        """è®¡ç®—æ–‡ä»¶MD5æ ¡éªŒå’Œ"""
        hash_md5 = hashlib.md5()
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def clean_to_long_format(self, summary_df, load_run_id):
        """å°†æ±‡æ€»æ•°æ®æ¸…æ´—ä¸ºæ ‡å‡†åŒ–é•¿æ ¼å¼"""
        observations = []
        
        # ä¿å­˜æ±‡æ€»æ–‡ä»¶ç”¨äºåç»­å‚è€ƒ
        summary_file = os.path.join(self.script_dir, 'copper_data', 'copper_daily_summary.csv')
        os.makedirs(os.path.dirname(summary_file), exist_ok=True)
        summary_df.to_csv(summary_file, index=False)
        raw_checksum = self.calculate_checksum(summary_file)
        
        metric_columns = {
            'Opening_Stock': 'lme_opening_mt',
            'Delivered_In': 'lme_delivered_in_mt',
            'Delivered_Out': 'lme_delivered_out_mt',
            'Closing_Stock': 'lme_closing_mt',
            'Open_Tonnage': 'lme_open_interest_mt',
            'Cancelled_Tonnage': 'lme_cancelled_mt'
        }
        
        for _, row in summary_df.iterrows():
            as_of_date = pd.to_datetime(row['Date'], format='%Y%m%d').strftime('%Y-%m-%d')
            
            opening = row.get('Opening_Stock')
            delivered_in = row.get('Delivered_In')
            delivered_out = row.get('Delivered_Out')
            closing = row.get('Closing_Stock')
            
            # è´¨é‡æ£€æŸ¥
            quality = 'ok'
            quality_notes = None
            
            if pd.isna(closing):
                quality = 'bad'
                quality_notes = 'Closing stock is null'
            else:
                if not pd.isna(opening) and not pd.isna(delivered_in) and not pd.isna(delivered_out):
                    balance_check = opening + delivered_in - delivered_out - closing
                    
                    if abs(balance_check) > 1e-6:
                        quality = 'warn'
                        quality_notes = f'Balance check failed: |opening + in - out - closing| = {abs(balance_check):.6f} > 1e-6'
            
            for col, metric_name in metric_columns.items():
                if col not in summary_df.columns:
                    continue
                
                value = row[col]
                
                if pd.isna(value):
                    continue
                
                obs = {
                    'metal': 'COPPER',
                    'source': 'LME',
                    'freq': 'D',
                    'as_of_date': as_of_date,
                    'metric': metric_name,
                    'value': value,
                    'unit': 'mt',
                    'is_imputed': False,
                    'method': 'daily',
                    'quality': quality,
                    'quality_notes': quality_notes,
                    'load_run_id': load_run_id,
                    'raw_file': 'copper_daily_summary.csv',
                    'raw_checksum': raw_checksum
                }
                
                observations.append(obs)
        
        clean_df = pd.DataFrame(observations)
        clean_df = clean_df.sort_values(['as_of_date', 'metric']).reset_index(drop=True)
        
        return clean_df
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if hasattr(self, 'driver'):
            self.driver.quit()
            print("\næµè§ˆå™¨å·²å…³é—­")
    
    def run(self, start_date, end_date, skip_download=False):
        """è¿è¡Œæ‰‹åŠ¨è·å–æµç¨‹
        
        å‚æ•°:
            start_date: å¼€å§‹æ—¥æœŸ (datetime æˆ– å­—ç¬¦ä¸² 'YYYY-MM-DD')
            end_date: ç»“æŸæ—¥æœŸ (datetime æˆ– å­—ç¬¦ä¸² 'YYYY-MM-DD')
            skip_download: æ˜¯å¦è·³è¿‡ä¸‹è½½æ­¥éª¤ï¼ˆä»…å¤„ç†å·²æœ‰æ–‡ä»¶ï¼‰
        """
        try:
            # è§£ææ—¥æœŸ
            if isinstance(start_date, str):
                start_date = datetime.strptime(start_date, '%Y-%m-%d')
            if isinstance(end_date, str):
                end_date = datetime.strptime(end_date, '%Y-%m-%d')
            
            print("\n" + "#"*60)
            print("# LME é“œæ•°æ®æ‰‹åŠ¨è·å–å·¥å…·")
            print(f"# è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"# ç›®æ ‡æ—¥æœŸèŒƒå›´: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
            print("#"*60)
            
            if not skip_download:
                # 1. è·å–æœ¬åœ°å·²ä¸‹è½½çš„æ–‡ä»¶
                local_files = self.get_local_downloaded_files()
                print(f"\næœ¬åœ°å·²æœ‰ {len(local_files)} ä¸ªæŠ¥å‘Šæ–‡ä»¶")
                
                # 2. è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„åœ¨çº¿æŠ¥å‘Š
                target_reports = self.fetch_reports_in_range(start_date, end_date)
                
                if not target_reports:
                    print("\næœªæ‰¾åˆ°ä»»ä½•æŠ¥å‘Š")
                    return None
                
                print(f"\nç›®æ ‡æ—¥æœŸèŒƒå›´å†…å…±æœ‰ {len(target_reports)} ä¸ªæŠ¥å‘Š")
                
                # 3. ç­›é€‰éœ€è¦ä¸‹è½½çš„æ–°æŠ¥å‘Š
                new_reports = []
                for report in target_reports:
                    filename = self.sanitize_filename(report['title']) + '.xls'
                    if filename not in local_files:
                        report['filename'] = filename
                        new_reports.append(report)
                
                if new_reports:
                    print(f"\nå‘ç° {len(new_reports)} ä¸ªæ–°æŠ¥å‘Šéœ€è¦ä¸‹è½½:")
                    for i, report in enumerate(new_reports, 1):
                        date_str = report['date'].strftime('%Y-%m-%d')
                        print(f"  {i}. {report['title']} ({date_str})")
                    
                    print("\nå¼€å§‹ä¸‹è½½...")
                    success_count = 0
                    for i, report in enumerate(new_reports, 1):
                        print(f"\n[{i}/{len(new_reports)}]")
                        if self.download_file(report['url'], report['filename']):
                            success_count += 1
                        time.sleep(0.5)
                    
                    print(f"\nä¸‹è½½å®Œæˆ! æˆåŠŸ: {success_count}/{len(new_reports)}")
                else:
                    print("\næ‰€æœ‰æŠ¥å‘Šéƒ½å·²ä¸‹è½½")
            else:
                print("\nè·³è¿‡ä¸‹è½½æ­¥éª¤ï¼Œä»…å¤„ç†å·²æœ‰æ–‡ä»¶...")
            
            # 4. å¤„ç†æŠ¥å‘Šæå–æ•°æ®ï¼ˆä»…å¤„ç†æŒ‡å®šæ—¥æœŸèŒƒå›´ï¼‰
            print("\n" + "="*60)
            print("å¼€å§‹å¤„ç†æŠ¥å‘Šå¹¶æå–æ•°æ®...")
            print("="*60)
            
            summary_df = self.process_downloaded_reports(start_date, end_date)
            
            if summary_df is None or summary_df.empty:
                print("æœªèƒ½æå–åˆ°æœ‰æ•ˆæ•°æ®")
                return None
            
            print(f"\næˆåŠŸæå– {len(summary_df)} æ¡æ—¥æœŸè®°å½•")
            print(f"æ—¥æœŸèŒƒå›´: {summary_df['Date'].min()} è‡³ {summary_df['Date'].max()}")
            
            # 5. æ¸…æ´—å¹¶è½¬æ¢ä¸ºé•¿æ ¼å¼
            load_run_id = datetime.now().strftime('%Y%m%d_%H%M%S')
            clean_df = self.clean_to_long_format(summary_df, load_run_id)
            
            print(f"\nç”Ÿæˆ {len(clean_df)} æ¡æ¸…æ´—åçš„è§‚æµ‹è®°å½•")
            
            # è´¨é‡æ±‡æ€»
            quality_summary = clean_df['quality'].value_counts()
            print("\nè´¨é‡æ£€æŸ¥ç»“æœ:")
            for quality, count in quality_summary.items():
                status = "âœ…" if quality == 'ok' else ("âš ï¸" if quality == 'warn' else "âŒ")
                print(f"  {status} {quality}: {count} æ¡")
            
            # 6. ä¿å­˜æ¸…æ´—åçš„æ•°æ®
            output_file = os.path.join(self.script_dir, 'clean_observations.csv')
            clean_df.to_csv(output_file, index=False)
            print(f"\nğŸ’¾ æ¸…æ´—åæ•°æ®å·²ä¿å­˜è‡³: {output_file}")
            
            # 7. å­˜å…¥æ•°æ®åº“
            print("\nğŸ“¤ æ­£åœ¨å­˜å…¥æ•°æ®åº“...")
            with DatabaseSession("lme_manual_fetch.py") as db:
                db.save(clean_df)
            
            print("\n" + "="*60)
            print("âœ… LMEæ•°æ®æ‰‹åŠ¨è·å–å®Œæˆï¼ï¼ˆå·²åŒæ­¥åˆ°æ•°æ®åº“ï¼‰")
            print("="*60)
            
            return clean_df
            
        finally:
            self.close()


def main():
    """ä¸»å‡½æ•° - äº¤äº’å¼é€‰æ‹©æ—¥æœŸèŒƒå›´"""
    print("\n" + "="*60)
    print("LME é“œæ•°æ®æ‰‹åŠ¨è·å–å·¥å…·")
    print("="*60)
    
    # è·å–ç”¨æˆ·è¾“å…¥çš„æ—¥æœŸèŒƒå›´
    print("\nè¯·è¾“å…¥è¦è·å–çš„æ—¥æœŸèŒƒå›´:")
    
    while True:
        start_input = input("å¼€å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD, ä¾‹å¦‚ 2025-06-01): ").strip()
        try:
            start_date = datetime.strptime(start_input, '%Y-%m-%d')
            break
        except ValueError:
            print("æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    while True:
        end_input = input("ç»“æŸæ—¥æœŸ (æ ¼å¼: YYYY-MM-DD, ä¾‹å¦‚ 2025-11-30): ").strip()
        try:
            end_date = datetime.strptime(end_input, '%Y-%m-%d')
            if end_date < start_date:
                print("ç»“æŸæ—¥æœŸä¸èƒ½æ—©äºå¼€å§‹æ—¥æœŸï¼Œè¯·é‡æ–°è¾“å…¥")
                continue
            break
        except ValueError:
            print("æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    # æ˜¯å¦è·³è¿‡ä¸‹è½½
    skip_input = input("\næ˜¯å¦è·³è¿‡ä¸‹è½½æ­¥éª¤ï¼Œä»…å¤„ç†å·²æœ‰æ–‡ä»¶? (y/N): ").strip().lower()
    skip_download = skip_input == 'y'
    
    print(f"\nç¡®è®¤ä¿¡æ¯:")
    print(f"  æ—¥æœŸèŒƒå›´: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}")
    print(f"  è·³è¿‡ä¸‹è½½: {'æ˜¯' if skip_download else 'å¦'}")
    
    confirm = input("\næ˜¯å¦å¼€å§‹æ‰§è¡Œ? (Y/n): ").strip().lower()
    if confirm == 'n':
        print("å·²å–æ¶ˆ")
        return
    
    # æ‰§è¡Œè·å–
    fetcher = LMEManualFetcher(headless=False)
    result = fetcher.run(start_date, end_date, skip_download)
    
    if result is not None:
        print(f"\nå¤„ç†å®Œæˆï¼Œå…± {len(result)} æ¡è§‚æµ‹è®°å½•")
    else:
        print("\nå¤„ç†å¤±è´¥")


def fetch_range(start_date, end_date, headless=False, skip_download=False):
    """ä¾¿æ·å‡½æ•°ï¼šç›´æ¥è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„æ•°æ®
    
    ç”¨æ³•ç¤ºä¾‹:
        from lme_manual_fetch import fetch_range
        fetch_range('2025-06-01', '2025-11-30')
    """
    fetcher = LMEManualFetcher(headless=headless)
    return fetcher.run(start_date, end_date, skip_download)


if __name__ == "__main__":
    # å¦‚æœå‘½ä»¤è¡Œå‚æ•°æä¾›äº†æ—¥æœŸï¼Œç›´æ¥ä½¿ç”¨
    if len(sys.argv) >= 3:
        start_date = sys.argv[1]
        end_date = sys.argv[2]
        skip_download = '--skip-download' in sys.argv
        
        fetcher = LMEManualFetcher(headless=False)
        result = fetcher.run(start_date, end_date, skip_download)
        
        if result is not None:
            print(f"\nå¤„ç†å®Œæˆï¼Œå…± {len(result)} æ¡è§‚æµ‹è®°å½•")
        else:
            print("\nå¤„ç†å¤±è´¥")
    else:
        # å¦åˆ™è¿›å…¥äº¤äº’æ¨¡å¼
        main()
